{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756baa79-c2c5-40bf-b4aa-ef48762e1f38",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7f894d-e5ab-4893-be9b-8d4ce0ce6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/starminalush/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/starminalush/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/starminalush/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import HalvingGridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import download_dataset_from_kaggle\n",
    "from preprocessing import (\n",
    "    delete_stop_words,\n",
    "    lemmatize,\n",
    "    normalize,\n",
    "    tokenize_by_sentence,\n",
    "    tokenize_by_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6476cded-6b99-4066-8bf8-9a32b9371b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285bb8d-a7ce-4463-9aeb-21986bc6ae46",
   "metadata": {},
   "source": [
    "## Загружаем датасет "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bd6cd1-fa59-41cf-8243-ec7c7ba00869",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = download_dataset_from_kaggle(\n",
    "    \"ozlerhakan/spam-or-not-spam-dataset\", \"spam_or_not_spam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859a79e5-2fc1-47df-8a55-d8a9a2e806bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{dataset_path}/spam_or_not_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346fc3a-0395-42c9-a8a3-cd5723c26567",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250671f1-b313-4cc8-aaea-ecd0846fd3ce",
   "metadata": {},
   "source": [
    "Посмотрим, что есть в датасете, уберем nan значения, если они есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d621ae-4c66-4355-806f-7ff20fa96d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>martin a posted tassos papadopoulos the greek ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man threatens explosion in moscow thursday aug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klez the virus that won t die already the most...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in adding cream to spaghetti carbonara which ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  label\n",
       "0   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0\n",
       "1  martin a posted tassos papadopoulos the greek ...      0\n",
       "2  man threatens explosion in moscow thursday aug...      0\n",
       "3  klez the virus that won t die already the most...      0\n",
       "4   in adding cream to spaghetti carbonara which ...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4d74ac-eccc-44d2-9963-cf1baaa8ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.372740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  3000.000000\n",
       "mean      0.166667\n",
       "std       0.372740\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6dc6e62-e4a6-4372-93b1-02f43d06a28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"email\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dbb950-4b0e-478c-ac53-568d6fa206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2643a-8f0e-4d5f-af21-e4dabe253f97",
   "metadata": {},
   "source": [
    "## Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b88177e-1570-4edc-a3c5-529e1e8815f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str, lang: str) -> str:\n",
    "    sentences: str = tokenize_by_sentence(text, lang)\n",
    "    preprocessed_sentences: list[str] = []\n",
    "    for sentence in sentences:\n",
    "        normalized_text = normalize(sentence, lang)\n",
    "        tokens = tokenize_by_words(normalized_text, lang)\n",
    "        clean_tokens = delete_stop_words(tokens, lang)\n",
    "        lemmatized_tokens = lemmatize(clean_tokens)\n",
    "        preprocessed_sentences.append(\" \".join([token for token in lemmatized_tokens]))\n",
    "    return \" \".join(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02bedb58-b8f4-40a9-8ece-6589ce1c7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"email\"] = df[\"email\"].apply(lambda x: preprocess(x, \"eng\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced6b55a-d6b8-40e1-8937-14c1edd93ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    date wed number aug number number number numbe...\n",
       "1    martin posted tassos papadopoulos greek sculpt...\n",
       "2    man threatens explosion moscow thursday august...\n",
       "3    klez virus die already prolific virus ever kle...\n",
       "4    adding cream spaghetti carbonara effect pasta ...\n",
       "Name: email, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"email\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e1f45-3aba-4842-bdd2-e9217b9057f3",
   "metadata": {},
   "source": [
    "## Способы векторизации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c9187-a7ea-4b93-aff7-6483a416f8bb",
   "metadata": {},
   "source": [
    "Рассмотрим TfidfVectorizer и CountVectorizer из sklearn как способы векторизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ea3552-f0db-46fa-a235-155cf14769cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vectorizer_info(\n",
    "    vectorizer, text: list[str] | pd.Series | pd.DataFrame\n",
    ") -> None:\n",
    "    embedding = pd.DataFrame(\n",
    "        vectorizer.fit_transform(text).toarray(),\n",
    "        columns=vectorizer.get_feature_names_out(),\n",
    "    )\n",
    "    print(f\"Vectorizer class {vectorizer.__class__}\\n\")\n",
    "    print(f\"Vectorizer embedding: {embedding}\\n\")\n",
    "    print(f\"Vocabulary info: {vectorizer.vocabulary_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999e837c-0482-4d52-af2d-d38de3cd6712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer class <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "\n",
      "Vectorizer embedding:    able  actually  admiring  ago  alexander  amphitheatre  athos  aug  behind  \\\n",
      "0     1         1         0    1          0             0      0    1       0   \n",
      "1     0         0         1    0          1             1      1    0       1   \n",
      "\n",
      "   car  ...  version  vircio  weather  wed  well  wide  without  workers  \\\n",
      "0    0  ...        3       1        0    1     0     0        1        2   \n",
      "1    1  ...        0       0        1    0     1     1        0        0   \n",
      "\n",
      "   works  yahoo  \n",
      "0      1      0  \n",
      "1      0      2  \n",
      "\n",
      "[2 rows x 145 columns]\n",
      "\n",
      "Vocabulary info: {'date': 21, 'wed': 138, 'number': 87, 'aug': 7, 'chris': 10, 'garrigues': 45, 'cwg': 20, 'dated': 22, 'numberfanumberd': 88, 'deepeddy': 25, 'com': 12, 'message': 78, 'id': 57, 'tmda': 129, 'vircio': 136, 'reproduce': 107, 'error': 30, 'repeatable': 105, 'like': 67, 'every': 31, 'time': 127, 'without': 141, 'fail': 36, 'debug': 24, 'log': 72, 'pick': 95, 'happening': 52, 'exec': 32, 'inbox': 59, 'list': 70, 'lbrace': 66, 'subject': 122, 'ftp': 44, 'rbrace': 102, 'sequence': 118, 'mercury': 77, 'hit': 54, 'marking': 75, 'hits': 55, 'tkerror': 128, 'syntax': 124, 'expression': 35, 'int': 60, 'note': 86, 'run': 110, 'command': 14, 'hand': 51, 'delta': 26, 'comes': 13, 'obviously': 89, 'version': 135, 'nmh': 85, 'using': 134, 'compiled': 16, 'url': 132, 'sun': 123, 'mar': 74, 'ict': 56, 'relevant': 104, 'part': 93, 'mhparam': 79, 'seq': 117, 'sel': 115, 'since': 119, 'works': 143, 'actually': 1, 'one': 90, 'explicit': 34, 'line': 69, 'search': 114, 'popup': 98, 'get': 46, 'created': 17, 'kre': 65, 'ps': 101, 'still': 121, 'code': 11, 'form': 40, 'day': 23, 'ago': 3, 'able': 0, 'reach': 103, 'cvs': 19, 'repository': 106, 'today': 130, 'local': 71, 'routing': 109, 'issue': 61, 'think': 126, 'exmh': 33, 'workers': 142, 'mailing': 73, 'martin': 76, 'posted': 99, 'tassos': 125, 'papadopoulos': 91, 'greek': 48, 'sculptor': 112, 'behind': 8, 'plan': 96, 'judged': 63, 'limestone': 68, 'mount': 82, 'kerdylio': 64, 'miles': 80, 'east': 28, 'salonika': 111, 'far': 37, 'athos': 6, 'monastic': 81, 'community': 15, 'ideal': 58, 'patriotic': 94, 'sculpture': 113, 'well': 139, 'alexander': 4, 'granite': 47, 'features': 39, 'ft': 43, 'high': 53, 'wide': 140, 'museum': 84, 'restored': 108, 'amphitheatre': 5, 'car': 9, 'park': 92, 'admiring': 2, 'crowds': 18, 'planned': 97, 'mountain': 83, 'weather': 137, 'pretty': 100, 'fast': 38, 'yahoo': 144, 'groups': 50, 'sponsor': 120, 'dvds': 27, 'free': 42, 'join': 62, 'unsubscribe': 131, 'group': 49, 'send': 116, 'email': 29, 'forteana': 41, 'use': 133}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_vectorizer_info(CountVectorizer(), df[\"email\"].iloc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1f0b6d-ae1d-45b3-af4e-72d266d70a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer class <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "\n",
      "Vectorizer embedding:        able  actually  admiring       ago  alexander  amphitheatre     athos  \\\n",
      "0  0.028104  0.028104  0.000000  0.028104   0.000000      0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.104165  0.000000   0.104165      0.104165  0.104165   \n",
      "\n",
      "        aug    behind       car  ...   version    vircio   weather       wed  \\\n",
      "0  0.028104  0.000000  0.000000  ...  0.084311  0.028104  0.000000  0.028104   \n",
      "1  0.000000  0.104165  0.104165  ...  0.000000  0.000000  0.104165  0.000000   \n",
      "\n",
      "       well      wide   without   workers     works     yahoo  \n",
      "0  0.000000  0.000000  0.028104  0.056207  0.028104  0.000000  \n",
      "1  0.104165  0.104165  0.000000  0.000000  0.000000  0.208331  \n",
      "\n",
      "[2 rows x 145 columns]\n",
      "\n",
      "Vocabulary info: {'date': 21, 'wed': 138, 'number': 87, 'aug': 7, 'chris': 10, 'garrigues': 45, 'cwg': 20, 'dated': 22, 'numberfanumberd': 88, 'deepeddy': 25, 'com': 12, 'message': 78, 'id': 57, 'tmda': 129, 'vircio': 136, 'reproduce': 107, 'error': 30, 'repeatable': 105, 'like': 67, 'every': 31, 'time': 127, 'without': 141, 'fail': 36, 'debug': 24, 'log': 72, 'pick': 95, 'happening': 52, 'exec': 32, 'inbox': 59, 'list': 70, 'lbrace': 66, 'subject': 122, 'ftp': 44, 'rbrace': 102, 'sequence': 118, 'mercury': 77, 'hit': 54, 'marking': 75, 'hits': 55, 'tkerror': 128, 'syntax': 124, 'expression': 35, 'int': 60, 'note': 86, 'run': 110, 'command': 14, 'hand': 51, 'delta': 26, 'comes': 13, 'obviously': 89, 'version': 135, 'nmh': 85, 'using': 134, 'compiled': 16, 'url': 132, 'sun': 123, 'mar': 74, 'ict': 56, 'relevant': 104, 'part': 93, 'mhparam': 79, 'seq': 117, 'sel': 115, 'since': 119, 'works': 143, 'actually': 1, 'one': 90, 'explicit': 34, 'line': 69, 'search': 114, 'popup': 98, 'get': 46, 'created': 17, 'kre': 65, 'ps': 101, 'still': 121, 'code': 11, 'form': 40, 'day': 23, 'ago': 3, 'able': 0, 'reach': 103, 'cvs': 19, 'repository': 106, 'today': 130, 'local': 71, 'routing': 109, 'issue': 61, 'think': 126, 'exmh': 33, 'workers': 142, 'mailing': 73, 'martin': 76, 'posted': 99, 'tassos': 125, 'papadopoulos': 91, 'greek': 48, 'sculptor': 112, 'behind': 8, 'plan': 96, 'judged': 63, 'limestone': 68, 'mount': 82, 'kerdylio': 64, 'miles': 80, 'east': 28, 'salonika': 111, 'far': 37, 'athos': 6, 'monastic': 81, 'community': 15, 'ideal': 58, 'patriotic': 94, 'sculpture': 113, 'well': 139, 'alexander': 4, 'granite': 47, 'features': 39, 'ft': 43, 'high': 53, 'wide': 140, 'museum': 84, 'restored': 108, 'amphitheatre': 5, 'car': 9, 'park': 92, 'admiring': 2, 'crowds': 18, 'planned': 97, 'mountain': 83, 'weather': 137, 'pretty': 100, 'fast': 38, 'yahoo': 144, 'groups': 50, 'sponsor': 120, 'dvds': 27, 'free': 42, 'join': 62, 'unsubscribe': 131, 'group': 49, 'send': 116, 'email': 29, 'forteana': 41, 'use': 133}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_vectorizer_info(TfidfVectorizer(), df[\"email\"].iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ad911-511d-4a41-a727-a6001a996248",
   "metadata": {},
   "source": [
    "Вывод: и CountVectorizer, и TfidfVectorizer в себя хранят одинаковые словари-счетчики слов, но у них отличается представление эмбеддингов - CountVectorizer выдает просто частоту встречаемости слов, а TfidfVectorizer отражает важность слова для документа в корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcfd21-d8e9-4fad-a519-b5a298291a34",
   "metadata": {},
   "source": [
    "## Обучаем модели с подбором гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f06d4-7e84-4252-89b5-a2ccd5d094aa",
   "metadata": {},
   "source": [
    "Разобьем выборку на train и test подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb177fdf-afe0-4d64-a9fc-f12fee7fc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"email\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14766c52-0dfc-411b-8792-6b93b3062b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_params(vectorizer, classifier) -> dict[str, Any]:\n",
    "    \"\"\"Создает пайплайн для векторизатора и классификатора и подбирает гиперпараметры для этого пайплайна.\"\"\"\n",
    "    pipeline = Pipeline(\n",
    "        [(\"vectorizer\", vectorizer[\"model\"]), (\"classifier\", classifier[\"model\"])]\n",
    "    )\n",
    "    param_grid = {\n",
    "        f\"vectorizer__{key}\": value for key, value in vectorizer[\"param_grid\"].items()\n",
    "    }\n",
    "    param_grid.update(\n",
    "        {f\"classifier__{key}\": value for key, value in classifier[\"param_grid\"].items()}\n",
    "    )\n",
    "    grid_search = HalvingGridSearchCV(\n",
    "        pipeline, param_grid, cv=5, random_state=RANDOM_STATE\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    return {\n",
    "        \"Vectorizer\": vectorizer[\"model\"].__class__.__name__,\n",
    "        \"Classifier\": classifier[\"model\"].__class__.__name__,\n",
    "        \"Лучшие гиперпараметры\": grid_search.best_params_,\n",
    "        \"Лучшая оценка на тестовой выборке\": test_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099c0cf-0fdf-4cda-86df-5876140c2b7a",
   "metadata": {},
   "source": [
    "Создадим список всех векторизаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a2dd81-cb25-4a2e-8e29-44d146b6e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [\n",
    "    {\n",
    "        \"model\": TfidfVectorizer(),\n",
    "        \"param_grid\": {\n",
    "            \"max_features\": [100, 500, 1000],\n",
    "            \"norm\": [\"l1\", \"l2\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"model\": CountVectorizer(),\n",
    "        \"param_grid\": {\"ngram_range\": [(1, 1), (1, 2), (1, 3)]},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cfc97-be57-444f-8eef-0a8c82b14a76",
   "metadata": {},
   "source": [
    "Создадим список всех классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27f3c8b4-6f59-4cff-bcbc-bdcab3c1cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    {\n",
    "        \"model\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        \"param_grid\": {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"criterion\": [\"gini\", \"log_loss\", \"entropy\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"model\": LogisticRegression(solver=\"liblinear\", random_state=RANDOM_STATE),\n",
    "        \"param_grid\": {\"C\": [0.1, 1, 10], \"penalty\": [\"l1\", \"l2\"]},\n",
    "    },\n",
    "    {\"model\": BernoulliNB(), \"param_grid\": {\"alpha\": [0.1, 1, 10]}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0b7a87-050e-4377-b315-4b2784831033",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = list(product(vectorizers, classifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318329c9-1a1b-47be-91ac-4b798b972f0e",
   "metadata": {},
   "source": [
    "Запускаем поиск гиперпараметров по всем экспериментам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abfe7fe6-f737-4db5-9078-dab644f3cc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████████████████████▌                                                                                                                                                             | 1/6 [00:22<01:53, 22.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier': 'DecisionTreeClassifier',\n",
      " 'Vectorizer': 'TfidfVectorizer',\n",
      " 'Лучшая оценка на тестовой выборке': 0.95,\n",
      " 'Лучшие гиперпараметры': {'classifier__criterion': 'log_loss',\n",
      "                           'classifier__max_depth': 5,\n",
      "                           'vectorizer__max_features': 100,\n",
      "                           'vectorizer__norm': 'l2'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████████████████████                                                                                                                              | 2/6 [00:36<01:11, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier': 'LogisticRegression',\n",
      " 'Vectorizer': 'TfidfVectorizer',\n",
      " 'Лучшая оценка на тестовой выборке': 0.9866666666666667,\n",
      " 'Лучшие гиперпараметры': {'classifier__C': 10,\n",
      "                           'classifier__penalty': 'l2',\n",
      "                           'vectorizer__max_features': 500,\n",
      "                           'vectorizer__norm': 'l2'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 3/6 [00:49<00:46, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier': 'BernoulliNB',\n",
      " 'Vectorizer': 'TfidfVectorizer',\n",
      " 'Лучшая оценка на тестовой выборке': 0.9566666666666667,\n",
      " 'Лучшие гиперпараметры': {'classifier__alpha': 0.1,\n",
      "                           'vectorizer__max_features': 500,\n",
      "                           'vectorizer__norm': 'l2'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                               | 4/6 [01:17<00:40, 20.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier': 'DecisionTreeClassifier',\n",
      " 'Vectorizer': 'CountVectorizer',\n",
      " 'Лучшая оценка на тестовой выборке': 0.95,\n",
      " 'Лучшие гиперпараметры': {'classifier__criterion': 'gini',\n",
      "                           'classifier__max_depth': 5,\n",
      "                           'vectorizer__ngram_range': (1, 2)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 5/6 [02:01<00:28, 28.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier': 'LogisticRegression',\n",
      " 'Vectorizer': 'CountVectorizer',\n",
      " 'Лучшая оценка на тестовой выборке': 0.99,\n",
      " 'Лучшие гиперпараметры': {'classifier__C': 10,\n",
      "                           'classifier__penalty': 'l1',\n",
      "                           'vectorizer__ngram_range': (1, 3)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:14<00:00, 22.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier': 'BernoulliNB',\n",
      " 'Vectorizer': 'CountVectorizer',\n",
      " 'Лучшая оценка на тестовой выборке': 0.9716666666666667,\n",
      " 'Лучшие гиперпараметры': {'classifier__alpha': 0.1,\n",
      "                           'vectorizer__ngram_range': (1, 1)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for vectorizer, classifier in tqdm(experiments):\n",
    "    pprint(grid_search_params(vectorizer=vectorizer, classifier=classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83dfd6-9903-4070-81d6-59ac1763e945",
   "metadata": {},
   "source": [
    "Вывод: среди всех экспериментов самое лучшее сочетание моделей следующее:\n",
    "\n",
    "Classifier: LogisticRegression  \n",
    "Vectorizer: CountVectorizer  \n",
    "Лучшая оценка на тестовой выборке: 0.99  \n",
    "Лучшие гиперпараметры:  \n",
    "* classifier__C: 10  \n",
    "* classifier__penalty: 'l1',  \n",
    "* vectorizer__ngram_range: (1, 3)}  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
